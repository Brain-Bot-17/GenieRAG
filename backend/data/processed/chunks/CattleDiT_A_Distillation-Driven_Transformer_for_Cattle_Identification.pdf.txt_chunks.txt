IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
1
CattleDiT: A Distillation-Driven Transformer for
Cattle Identification
Niraj Kumar
Student Member, IEEE, and Sanjay Kumar Singh*, Senior Member, IEEE
Abstract—Rising standards for biosecurity, disease prevention,
and livestock tracing are driving the need for an efficient iden-
tification system within the livestock supply chain. Traditional
methods for cattle identification are invasive and unreliable
---
due to issues like fraud, theft, and duplication. While deep
learning-based methods, particularly Vision Transformers (ViTs),
have demonstrated superior accuracy compared to traditional
Convolutional Neural Networks (CNNs), but they require signif-
icantly larger datasets for training and have high computational
demands. To address the challenges of large data requirements
and to achieve faster convergence with fewer parameters, this
paper proposes a novel distillation-based transformer approach
---
for cattle identification. In this paper, we extract the muzzle
region from a publicly available front-face cattle image dataset
containing 300 cattle-face data and perform a distillation process
to ensure that the student transformer model effectively learns
from the teacher model through a proposed Adaptive Stochastic
Depth mechanism. The teacher model, based on a lightweight
custom convolutional network, extracts key features, which are
then used to train the student Vision Transformer model, named
---
CattleDiT. This approach reduces the data requirements and
computational complexity of the ViT while maintaining high
accuracy. The proposed model outperforms conventional ViT
models and other state-of-the-art methods, achieving 99.81%
accuracy on the training set and 96.67% on the test set.
Additionally, several Explainable AI methods are employed to
enhance interpretability of the prediction results.
Index Terms—Cattle identification, Livestock Farming, Vision
---
Transformer, Knowledge Distillation, Yolo v10
I. INTRODUCTION
R
ECENTLY, as the population continues to grow and
the demand for meat and dairy products rises, along-
side increasing expectations for top-tier food standards, the
livestock sector is transitioning from traditional, small-scale
farming to more intensive and specialized grazing practices.
Managing the welfare of a large number of individual cattle
is becoming increasingly complex. Vital elements of dairy
---
production and genetic improvement, such as breeding, cattle
selection, calving, and vaccination, rely on the identification
of individual cattle [1]. To enhance output and quality, there is
a definite need for automated livestock management to ensure
traceability and uphold product excellence.
Conventional methods of managing herds can be labor-
intensive and invasive, possibly leading to discomfort and
health issues for livestock [2]. To track and identify individual
---
animals within herds, a range of techniques has been utilized,
such as ear tagging and tattooing, hot ironing, freeze branding,
*Corresponding Author
Niraj Kumar, and Sanjay Kumar Singh is with the Department of CSE,
IIT(BHU), Varanasi 221005, India. (E-mail: nirajkumar.rs.cse22@itbhu.ac.in;
sks.cse@iitbhu.ac.in
collar IDs, and visual markers like painting in the body of an
individual animal, but the traditional methods like tagging,
tattooing, and freeze branding can result in damage and con-
---
siderable discomfort to the livestock. Some methods, like GPS-
based collar straps and microchipping, need careful design
and placement to minimize injury, as well as microchipping
involves subcutaneous injections that can be harmful to the
cattle’s well-being [3], [4].
Fig. 1. Unique biometric feature in cattle muzzle
Presently, available cattle identification methods exhibit
limitations. However, recent advancements in biometrics tech-
nology and research, such as facial recognition systems, of-
---
fer promising alternatives for cattle identification, providing
less invasive monitoring and a wide range of benefits and
applications like livestock registration, ownership verification,
tracking of cattle, and further research. For example, A novel
Jinnan individual cattle recognition approach, based on a
mutual attention learning scheme proposed by [5], significantly
improves identification accuracy by leveraging deep learning
to focus on the most distinctive features of each cow’s face,
---
enhancing reliability even in challenging conditions like vary-
ing lighting and partial occlusions. Further, [6] proposed an
algorithm that locates key areas of a cow’s face and refines the
identification process by concentrating on regions that are most
distinctive and stable over time, thus improving robustness
and accuracy in real-world farming conditions. According
to [7], these advanced biometric technologies enable more
accurate and efficient livestock registration, secure ownership
---
verification, precise cattle tracking, and enhanced research
into cattle behavior and health. Moreover, integrating facial
recognition with body measurement technologies can improve
biosecurity measures, reduce labor costs, and enhance overall
livestock management, ensuring both animal well-being and
the economic efficiency and sustainability of livestock farm-
ing.
One particularly important feature for cattle identification
---
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
2
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
TABLE I
SUMMARY OF PREVIOUS STUDIES ON CATTLE IDENTIFICATION.
Author’s
Feature
Cattle Counts
Total Images
Methods
Accuracy (%)
Andrew et al. [8]
Body
92
377
ASIFT, SVM
97
Gaber et al. [9]
Muzzle
31
217
WLD, AdaBoost
98.9
Kumar et al. [10]
Muzzle
545
5000
LBP. ANN
96.7
Awad et al. [2]
Muzzle
15
105
SURF, SVM
93
Shojaeipour et al. [4]
Muzzle
300
2900
Fine-Tuned ResNet50
99.11
Xu et al. [1]
Face
90
2318
RetinaFace, ArcFace
91
Bhole et al. [11]
---
90
2318
RetinaFace, ArcFace
91
Bhole et al. [11]
Body
383
3694
Contour Maps
99.64
Dulal et al. [12]
Muzzle
300
2900
Yolo v5
95
Gong et al. [13]
Face
5677
SK-ResNet
98.42
Weng et al. [14]
Face
130
18200
Two Branch CNN
99.71
Lee et al. [15]
Muzzle
336
9230
EfficientNetV2
97.00
Xu et al. [16]
Face
120
4000
Embedding Optimization, RetinaFace
93.45
Xiao et al. [17]
Body
150
5000
YOLOX, CowbodyNet
94.70
is the cattle muzzle. As shown in Fig. 1, the muzzle of a
---
cow, which includes unique patterns of beads and ridges, is
as distinctive as human fingerprints. This uniqueness makes
it an invaluable feature for identification purposes. Unlike
other methods that might rely on tags or branding, which
can be lost or tampered with, the natural features of the
muzzle provide a consistent and reliable means of identifying
individual cattle. In comparison, body-based or facial features
are less consistent due to factors such as growth, injury,
---
or environmental conditions that can alter visual appearance
over time. The beads and ridges on a cow’s muzzle remain
relatively unchanged throughout the animal’s life, making it
a stable biometric marker. By incorporating the cattle muzzle
into identification systems, the accuracy and reliability of cattle
identification can be significantly enhanced, ensuring better
tracking and management of livestock.
The emergence of advanced deep learning techniques, such
---
as Convolutional Neural Networks (CNNs), has significantly
enhanced feature extraction capabilities [18], proving par-
ticularly advantageous for cattle identification by effectively
capturing and distinguishing intricate patterns in the ani-
mals’ facial features. State-of-the-art deep learning models
like ResNet50 [19] and Inception V3 [20] have demonstrated
promising results in terms of accuracy due to their ability
to learn deep hierarchical representations of the input data.
---
However, the downside of these models is their substantial
parameter count and computational demands, requiring vast
amounts of data and significant processing power to achieve
optimal results. This high resource requirement presents a
challenge for implementation in resource-constrained systems
typically found in agricultural settings, where computational
resources and large annotated datasets may be limited. Despite
these challenges, ongoing research is focusing on developing
---
more efficient versions of these models, such as lightweight
CNNs and knowledge distillation techniques, to make deep
learning-based cattle identification more accessible and prac-
tical for real-world applications.
As deep learning technology advances, there has been
significant progress not only in convolutional models but also
in transformer models, which have shown superior perfor-
mance in various tasks, including classification, detection, and
---
identification. Vision Transformers (ViTs), for instance, are
cutting-edge image-based transformer models that leverage the
self-attention mechanism to process image patches, offering
a novel approach to image recognition tasks [21]. Unlike
CNNs, which use convolutional layers to extract features
hierarchically, Vision Transformers treat images as sequences
of patches and apply transformer architectures to capture
global context and dependencies across the entire image.
---
This method allows ViTs to achieve comparable or even
superior accuracy with less data augmentation and simpler pre-
processing techniques. However, while Vision Transformers
require more data than traditional CNN models for effective
training, they also demand substantial computational resources
and specialized hardware, such as high-performance GPUs or
TPUs, due to their complex architecture and extensive parallel
processing needs. Additionally, the training process for ViTs
---
can be computationally intensive, often necessitating large-
scale distributed training environments to manage the high-
dimensional data and extensive parameter space efficiently.
Despite these challenges, Vision Transformers represent a
promising direction in deep learning, with ongoing research
focusing on optimizing their efficiency and making them
more accessible for various applications, including those in
resource-constrained environments.
In this study, we are trying to solve the issue of high data
---
and computational demands by deep learning based methods
in cattle identification systems by introducing a novel deep
learning-based approach. We utilize knowledge distillation to
reduce the computational overhead required for training Vision
Transformer models. Additionally, we propose a distillation-
based Transformer model with adaptive stochastic depth,
which not only reduces the number of parameters but also
improves the training process by selectively skipping layers
---
during each forward pass. This mechanism enhances both
convergence speed and model accuracy, making the approach
more suitable for resource-constrained environments. To sum
up, the significant contributions of this paper are as follows.
1) A novel approach for cattle identification is proposed,
utilizing a deep learning-based method called CattleDiT,
which employs a teacher-student strategy to train the
model efficiently.
2) An efficient approach for cattle muzzle detection is im-
---
plemented using the state-of-the-art Yolo v10 algorithm,
with an added cropping mechanism to extract muzzle
images.
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
---
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
3
Fig. 2. Cattle identification workflow using our proposed methodology
3) The proposed method incorporates a novel adaptive
stochastic depth mechanism with token mixing in the
transformer block, aiding in effective regularization for
a better training process.
4) The proposed model is compared with several state-of-
the-art models, demonstrating superior performance.
---
5) Additionally, various Explainable AI methods are uti-
lized for better interpretation of the predicted results.
Fig. 2. illustrates the comprehensive workflow of the cattle
identification process outlined in this paper. The structure of
this paper is as follows: Section II provides a brief overview
of related works in the field of cattle identification. Section
III outlines the methods and techniques used to develop our
model. Section IV offers a detailed analysis and experimental
---
evaluation of our model, including comparisons with other
state-of-the-art models. Finally, Section V concludes the paper,
discussing limitations and future directions.
II. BACKGROUND AND RELATED WORK
A. Cattle Identification
Each of these methods, while advancing cattle identification,
has practical limitations that need addressing for robust, scal-
able solutions in real-world settings. A detailed comparison
of machine learning and deep learning models is presented in
Table I.
B. Vision Transformers
---
Table I.
B. Vision Transformers
Vision Transformers have been recently applied to various
identification and classification tasks. Touvron et al. presented
a Data Efficient Vision Transformer that trains with less data
using a distillation process and existing data augmentation
strategies from convolutional neural networks. However, it
relies heavily on a large pretrained neural network for teacher
training, limiting flexibility and adaptability to diverse datasets
---
[22]. Shahchera et al. proposed a Vision Transformer for
traffic sign detection using the GTSDB dataset, achieving
superior performance compared to state-of-the-art methods.
Despite this, it struggled with varying real-world traffic sign
conditions and occlusions [23]. Yin et al. introduced the
SMIL-DeiT network for Alzheimer’s disease classification,
using self-distillation and multiple instance learning, but faced
challenges with the complexity of self-supervised learning
---
and high computational resource requirements [24]. Alotaibi
et al. suggested an ensemble classification model integrating
Data Efficient Vision Transformer and pre-trained ViT with a
soft voting technique, improving breast cancer classification
accuracy. However, the model’s complexity and high com-
putational cost pose practical challenges [25]. While Vision
Transformers show promise in identification and classification
tasks, they face limitations such as dependency on large
---
pretrained models, challenges with data variability, and high
computational costs, hindering practical applications.
C. Knowledge Distillation
Knowledge distillation (KD) has emerged as a powerful
technique for transferring knowledge from a large, high-
performing teacher model to a smaller, lightweight student
model, enabling efficient deployment without compromising
performance. Introduced by Hinton et al. [26], KD involves
minimizing the divergence between the soft predictions of the
---
teacher and the student, encouraging the student model to
mimic the teacher’s behavior. This method has been widely
adopted in various domains for model compression and ac-
celeration. In recent years, several advancements have been
proposed to improve the distillation process by leveraging
modifications to teacher-student architectures and loss func-
tions. For instance, Touvron et al. [27] demonstrated the
integration of KD with Vision Transformers (ViTs), where a
---
convolutional neural network (CNN) served as the teacher to
guide the training of a ViT-based student, achieving state-of-
the-art performance in image classification tasks. Similarly,
Heo et al. [28] introduced a comprehensive approach to
enhance distillation by incorporating feature-level and logit-
level knowledge transfer, effectively improving the student’s
capacity to generalize.
III. PROPOSED METHOD
This section provides a concise overview of the techniques
---
utilized in dataset preparation and the proposed methodology
for cattle identification. Initially, we train, tune, and test
our model on the dataset that has been described in the
relevant subsection below. This section also provides a detailed
breakdown of the experimental dataset partition used in the
study.
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and
---
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
---
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
4
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
TABLE II
CATTLE COUNT BY BREED AND GEOGRAPHICAL REGION USED IN THIS
STUDY
Breed
Geographical Region
Cattle Count
Angus
Scotland
153
European
Europe
110
Hereford
England
37
Deshi (Cross)
India
16
Haryanvi
India
09
Jarshi
India
10
Sahiwal
India
08
Halstein Friesian
Netherland
07
Mixed Breeds
459
US Feedlot Cattle
United States
268
A. Data Acquisition and Preprocessing
In this study, we utilized a publicly accessible dataset
---
mentioned in paper [4] comprising 2900 images of 300 cattle,
encompassing three breeds: Bos taurus beef cattle with mixed
breeds such as Hereford Charolais, Angus, and Simmental.
The images of the cattle are uniform and of standard res-
olution. However, due to poor lighting conditions and head
movement of the cattle, some images in the dataset appear
blurry. Consequently, these images were excluded from the
preprocessing stage.
An overview of the cattle breeds, their geographical ori-
---
gins, and the corresponding cattle count used in our study
is provided in Table II. The dataset now includes a wider
variety of breeds such as Deshi, Haryanvi, Jarshi, and Sahiwal
from India, along with Holstein Friesian from the Nether-
lands. These additions complement the previously included
Angus (Scotland), European (Europe), and Hereford (England)
breeds. Furthermore, 459 samples categorized under ”Mixed
Breeds” represent cattle from varied genetic backgrounds,
---
ensuring additional diversity. The dataset also incorporates 268
cattles of U.S. feedlot to account for variability in commercial
farming conditions. This diverse dataset was created to en-
hance the robustness and generalizability of our model across
different breeds and environmental conditions. The newly
added dataset was collected using a smartphone equipped with
a 50MP camera capable of capturing high-resolution videos
(1080p at 60fps). From these videos, snapshots were extracted
---
to create high-quality muzzle images (4000 x 6000 pixels,
RGB, JPEG format). Data collection was conducted during
optimal sunlight conditions (between 7:30 AM and 4:30 PM)
to ensure clear and consistent imaging.
In our work we applied some image augmentation and
transformation techniques to introduce variation in data and
also increase the number of images. We maintained certain
configurations, incorporating minor zoom adjustments of 0.01
factor, brightness variations within a defined range between 0.5
---
to 0.9, slight random rotations up to 2 degrees, subtle shifts in
color channels, and the utilization of ZCA whitening is applied
for dimensionality reduction in the images. Furthermore, the
settings permit minor horizontal and vertical shifts along all
the images. A few samples of the dataset are shown in Fig. 3.
Fig. 3.
Sample cattle images and their corresponding extracted muzzle
patterns. (a) Muzzle patterns extracted from images in a public dataset. (b)
---
Muzzle patterns extracted from videos in our collected dataset.
B. Data Partitioning
The dataset comprises a total of 3892 images after aug-
mentation and transformation, which are divided into three
distinct subsets: training, validation, and testing. The training
set contains 2096 images, accounting for 54% of the total
dataset. The validation set includes 896 images, representing
23%, and the testing set consists of 900 images, also making
up 23% of the dataset. The dataset features 300 unique classes,
---
numerically labeled between 1 and 354. Due to some missing
labels within this range, the final number of classes is 300
rather than 354. Importantly, the partitioning is done in a class-
disjoint manner, ensuring that images of the same class are not
shared across the training, validation, and testing subsets. This
eliminates any potential overlap, preventing biased evaluations
and ensuring that the performance of the model is accurately
assessed on unseen classes.
C. Detection and Cropping
---
C. Detection and Cropping
An important step in cattle biometric identification is the
accurate detection and extraction [29] of the muzzle region.
To address this challenge, we employed the Yolo v10 object
detector [30] with transfer learning to adapt the network
weights for muzzle detection. Unlike previous Yolo versions
and traditional methods that has a limitation of scanning an
image multiple times to detect objects, Yolo v10 partitions
the input image into a grid and, in a single pass through the
---
network, forecasts bounding boxes and class probabilities for
every grid cell [31], [32]. This method not only simplifies the
detection process but also reduces computational overhead,
making it ideal for real-time applications. Each grid cell
predicts multiple bounding boxes, alongside their confidence
scores and class probabilities, ensuring a comprehensive scan.
To refine these predictions, Yolo v10 employs non-maximum
suppression, a technique that eliminates overlapping boxes,
---
thus sharpening the accuracy of object detection. Following
the successful training of the model on the muzzle images,
additional steps involve cropping the muzzle to extract specific
muzzle images. Subsequently, data augmentation is applied,
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and
---
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
---
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
5
Fig. 4. Knowledge Distillation on CNN teacher and Transformer Student
TABLE III
SUMMARY OF CUSTOM CNN MODEL USED FOR TEACHER’S TRAINING
Layers
Input size
Stride
Output size
Parameters
Conv1
128x128x3
1
126x126x32
896
Conv2
126x126x32
1
124x124x32
9248
Conv3
124x124x32
2
62x62x32
9248
Conv4
62x62x32
1
60x60x64
18496
Conv5
60x60x64
1
58x58x64
36928
Conv6
58x58x64
2
29x29x64
36928
GAP
29x29x64
1x1x64
Dense
64
300
19500
---
36928
GAP
29x29x64
1x1x64
Dense
64
300
19500
Total
131,244
and further image transformations are implemented to improve
the model’s performance and robustness.
D. Knowledge Distillation
In this work, the knowledge distillation approach is involved
in transferring the muzzle feature knowledge from a custom-
designed CNN model (referred to as the teacher) to a trans-
former model (referred to as the student) [26].
Table III provides an overview of the encoder utilized
---
in teacher’s training, comprising six convolutional layers, a
Global Average Pooling (GAP) layer, and a Dense layer. The
process of knowledge distillation is shown in Fig. 4.
E. Distillation based Transformer
This CattleDiT model addresses the challenge of training
on large-scale muzzle image datasets with a smaller number
of parameters, making it efficient for resource-constraint hard-
ware and faster convergence. The proposed method involves
three fundamental components: input features, training on fea-
---
tures, and classification. Input features include three primary
tokens: class tokens, patch tokens obtained from the input
muzzle image as shown in Fig. 5, and distillation tokens.
Class tokens are embeddings derived from the initial training
of the teacher model, while patch tokens are obtained from
the input muzzle image. Distillation tokens are designed to
replicate the label predicted by the teacher model and are used
similarly to class tokens in conjunction with other embeddings
---
through self-attention. In this approach, we integrate Adaptive
Stochastic Depth and token mixing to improve performance.
1) Adaptive Stochastic Depth: This regularization tech-
nique dynamically adjusts the drop probability of network
layers during training. Traditional stochastic depth randomly
drops entire layers during training, which can improve the
network’s generalization ability. However, Adaptive Stochastic
Depth takes this a step further by making the drop prob-
---
ability a learnable parameter. This allows the network to
adaptively learn the optimal drop rate for each layer. The drop
probability is adjusted based on the training process, which
can help in mitigating overfitting and improving the model’s
performance on unseen data. The implementation ensures that
during training, if the drop probability is high, the layer’s
output is zeroed out, effectively skipping the layer. Otherwise,
the output is scaled appropriately to maintain the expected
---
activation statistics. Additionally, Adaptive Stochastic Depth
adds specific value to the task of cattle identification for
reasons like:
1) Overfitting Mitigation: Cattle muzzle identification re-
lies on subtle, discriminative features, and the dataset
used is not excessively large. Traditional ViT models
are prone to overfitting on smaller datasets. Adaptive
Stochastic Depth introduces a regularization mechanism
that dynamically adjusts drop probabilities, reducing
---
overfitting without the need for extensive data augmen-
tation or larger datasets.
2) Model Efficiency: As we aim to improve the efficiency
of ViT models, Adaptive Stochastic Depth helps to
reduce computational complexity during training by
allowing the model to skip layers adaptively. This
has shown to accelerate training while preserving the
model’s performance, an important consideration given
the computational demands of ViT models.
2) Token Mixing: This technique is used here to enhance
---
representation learning by combining the outputs of the self-
attention layer with the original input tokens. In a typical
transformer model, the self-attention mechanism computes the
relationships between different tokens, allowing the model to
capture contextual information. By adding the original input
tokens back to the attention output, token mixing reinforces
the original features while incorporating the learned attention
representations. This additive operation can help in retaining
---
the essential characteristics of the input data while benefiting
from the contextual understanding provided by self-attention.
This process can improve the overall robustness and effective-
ness of the model in learning meaningful representations from
the data.
Enhanced Self-Attention with Adaptive Stochastic Depth
and Token Mixing can be derived using the original attention
mechanism introduced by [21] and shown in Eq. (1).
Attention(Q, K, V ) = σ
QKT
√dk

V
(1)
---
Attention(Q, K, V ) = σ
QKT
√dk

V
(1)
Here, σ represents the softmax function. Q, K, and V
represent the query, key, and value, respectively. In the context
of cattle muzzle image classification, Q represents the query
patches (tokens) extracted from the muzzle image of the
particular cattle to be classified, K represents the key patches
from the same image (or batch of images), and V represents
the corresponding value patches. By taking the dot product
---
of Q and K (transposed), divided by the square root of the
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
6
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
Fig. 5. The CattleDiT framework combines a CNN-based teacher and a Transformer-based student, where soft labels from the teacher guide the student via
a distillation token. The student processes patch, class, and distillation tokens through Transformer blocks with Adaptive Stochastic Depth and token mixing.
Training is supervised by both distillation and cross-entropy losses.
dimensionality of the features dk, the attention mechanism
---
measures how much attention to give to each patch of the
input image based on its similarity to the query patches. This
local patch-wise attention helps in capturing the important
relationships within the muzzle image. After computing the
attention output, we apply Adaptive Stochastic Depth (ASD):
ASD(X) = X · b + 0 · (1 −b)
(2)
where b is a Bernoulli random variable with parameter p (the
learnable drop probability). During training, this step effec-
---
tively drops or retains the layer’s output based on the learned
probability. The token mixing process involves combining the
attention output with the original input tokens:
TM(X, Y ) = X + Y
(3)
where TM represents the token mixing operation, X is
the attention output, and Y is the original input. This addi-
tive operation enhances representation learning by integrating
the context-aware representations from self-attention with the
original features.
---
original features.
Putting it all together, the enhanced self-attention with
Adaptive Stochastic Depth and token mixing can be formu-
lated as:
(4)
Attention(Q, K, V, Xinput) =

σ
QKT
√dk

V · b

+ 0 · (1 −b) + Xinput
In this formulation, σ represents the softmax function,
which computes the relevance of each image patch in relation
to the query patches. b is a Bernoulli random variable with
parameter p, used in Adaptive Stochastic Depth to dynamically
---
drop or keep layer outputs. TM(X, Y ) denotes the token
mixing operation, which adds the original input tokens Y to
the attention output X, reinforcing the learned representations.
ASD(X) applies adaptive stochastic depth to the input X,
providing dynamic dropout during training.
Finally, the classification process involves two loss func-
tions: cross-entropy loss is used for training the token-based
classifier in a supervised manner, while distillation loss is
---
used during training to leverage knowledge from a larger
teacher model by matching the output distributions of the two
models. This helps the student model generalize better and
achieve higher performance. The mathematical intuition can
be expressed as:
LDistil
global = 1
2LCE(ψ(Zs), y) + 1
2LCE(ψ(Zs), yt)
(5)
Here, Zs represents the logits produced by the student
model for cattle classes, and ψ(Zs) is a probability distri-
bution over the classes. y and yt represent true and soft
---
labels, respectively. By averaging these two losses, the formula
ensures that the student model is trained not only to predict
the true labels accurately but also to mimic the probability
distribution provided by the teacher model, which can help im-
prove its generalization ability and identification performance.
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and
---
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
---
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
7
The complete process of training and testing the proposed
distillation-based transformer is explained in Algorithm 1.
Algorithm 1 Training of CattleDiT
1: Input: Training dataset Dtrain, Validation dataset Dval,
Test dataset Dtest
2: Initialize Model and Training Parameters
3: Initialize Lightweight CNN as Teacher model T
4: Initialize ViTClassifier as Student model S
5: Initialize
Distillation based Transformer
---
5: Initialize
Distillation based Transformer
with Student and Teacher
6: procedure TRAINCNN(Dtrain, Dval)
7:
for each epoch e from 1 to n do
8:
for each batch b do
9:
Extract images xb and labels yb from batch b
10:
Perform forward pass: zb = T(xb)
11:
Compute loss: Lb = L(zb, yb)
12:
Backpropagate the gradients: ∇Lb
13:
Update the model weights: θT ←θT −η∇Lb
14:
end for
15:
end for
16: end procedure
17: procedure TRAINCATTLEDIT(Dtrain, Dval)
18:
for each epoch e from 1 to n do
19:
for each batch b do
20:
---
19:
for each batch b do
20:
Extract images xb and labels yb from batch b
21:
forward pass on student model: zS = S(xb)
22:
forward pass on trained teacher: zT = T(xb)
23:
Compute student loss: Lstudent = L(zS, yb)
24:
Compute distillation loss:
Ldistill = LKD(zS, zT )
25:
Compute total loss:
Ltotal = αLstudent + (1 −α)Ldistill
26:
Backpropagate the gradients: ∇Ltotal
27:
Update weights: θS ←θS −η∇Ltotal
28:
Log train and validation metrics
29:
end for
30:
end for
31: end procedure
---
29:
end for
30:
end for
31: end procedure
32: procedure TESTCATTLEDIT(Dtest)
33:
for each sample image xi in Dtest do
34:
Predict class of xi: Ci = arg max S(xi)
35:
Log test metrics
36:
end for
37: end procedure
IV. EXPERIMENTS AND RESULTS
A. Experimental Setup
All experiments were performed on a personal computer
according to the specifications outlined in Table IV. Both
CPU based and GPU-based architectures were employed for
evaluating variances in results between the proposed method
---
and other state-of-the-art approaches.
The model has been trained using the hyperparameters
listed in Table V. After testing various values, the chosen
TABLE IV
CONFIGURATION SETUP USED IN OUR WORK FOR ALL THE EXPERIMENTS
Name
Configurations
Platform
Linux-based OS
CPU
AMD Ryzen Processors (2.4 GHz)
Memory
16 GB DDR4 RAM
Hard Disk
1 TB Storage
GPU
Nvidia Geforce Rtx 4GB vram
GPU Accelerator
Cuda
Deep Learning Framework
TensorFlow, Keras
---
Cuda
Deep Learning Framework
TensorFlow, Keras
Fig. 6. Yolo v10 muzzle detection results on the original cattle face dataset
parameters for the proposed model resulted in better accuracy.
For comparison with other state-of-the-art models, standard
values for the hyperparameters were used.
TABLE V
HYPERPARAMETER DETAILS USED IN OUR WORK FOR ALL THE
EXPERIMENTS
Type
Hyperparameters
Value
Resolution
128
Patch Size
16
Patches
196
Proposed
Layer Norm. Epsilon
1 × 10−6
Model
Projection Dimension
192
Heads
3
---
1 × 10−6
Model
Projection Dimension
192
Heads
3
Layers
12
MLP Units
[768, 192]
Dropout Rate
0.0001
Drop Path Rate
0.1
Training
Learning Rate
0.0005
Weight Decay
0.0001
Data
Batch Size
32
Classes
300
B. Performance Analysis
In this section, the performance and evaluation outcomes
alongside an ablation study on transformers are presented. Our
proposed method was evaluated on performance metrics such
as accuracy, f1-score, precision, recall, and parameter count.
---
Fig. 6. illustrates the detection results of the muzzle using
Yolo v10. During the muzzle extraction process from front-
face cattle images, about 800 images from the dataset are
employed to train the Yolo model. Prior to model training,
annotation is necessary for the model to recognize the region
of interest. After manually annotating all images, the model
---
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
8
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
TABLE VI
MUZZLE DETECTION RESULTS ON YOLO V10 ALGORITHM
Epochs
Box Loss
Class Loss
Df Loss
Precision
Recall
mAP50
mAP50-95
Train
Val
Train
Val
Train
Val
20
0.88
0.74
0.74
0.44
1.22
1.24
0.97
0.99
0.99
0.80
40
0.65
0.56
0.45
0.28
1.05
1.07
0.98
1
0.99
0.85
60
0.52
0.41
0.34
0.23
0.98
0.94
0.98
1
0.99
0.92
80
0.40
0.33
0.27
0.19
0.92
0.89
0.99
1
0.99
0.95
100
0.28
0.25
0.18
0.15
0.85
0.85
0.99
1
0.99
0.97
---
0.28
0.25
0.18
0.15
0.85
0.85
0.99
1
0.99
0.97
has shown improved performance in providing all dataset
images for testing. Additionally, we implemented a cropping
mechanism to isolate and store the muzzle parts. All the
training results of the Yolo v10 algorithm for muzzle detection
are illustrated in Table VI. Here, Box loss measures the
error between the predicted bounding boxes and the ground
truth bounding boxes for the muzzle. Class loss evaluates the
---
error in predicting the correct class labels for the detected
muzzle. Distribution Focal Loss (DFL) enhances the precision
of bounding box regression by sharpening predictions for the
muzzle object. Additionally, mAP50 and mAP50-95 metrics
are used to evaluate performance. The results show that as the
loss values decrease and the mAP increases over 100 epochs,
indicating improved detection of the muzzle from the given
input image.
The training details of the teacher as well as the student
---
model are shown in Fig 7. The curves indicate that the model
performs well on the training data when trained for 50 epochs,
with metrics such as loss and accuracy being compared. As
shown in Fig. 7(a), the loss is depicted for three cases: the
student, teacher, and distillation processes. Similarly, in Fig.
7(b), upon training, the teacher model achieves an accuracy of
98.72% on the training set, while the student model achieves
an accuracy of 98.81%.
---
an accuracy of 98.81%.
Fig. 9 presents three interpretability techniques—Shapley
Additive Explanations (SHAP) [33], Integrated Gradients (IG)
[34], and GradCAM to analyze the model’s decision-making
process. The SHAP interpretation reveals that red pixels are
predominantly concentrated around unique ridge structures,
indicating their importance in classification. The IG attribution
mask provides further insight, with red circles marking the
regions that significantly influence positive predictions, while
---
blue circles denote areas with minimal impact. Additionally,
the Grad-CAM visualization highlights the most discriminative
regions in red and yellow, showing where the model focuses its
attention during prediction. This multi-faceted interpretability
approach offers a comprehensive understanding of how the
model processes and identifies distinct muzzle patterns.
The study presented in Table VII illustrates a detailed ab-
lation analysis on multiple subsets taken from the dataset and
---
different image resolutions. This table calculates three different
levels of K-Fold cross-validation based on two criteria: Top-
1 and Top-5 accuracy. The results show that higher image
resolutions generally achieve higher accuracy across different
class sizes. For example, the accuracy for images with a
resolution of 1282 is consistently higher than for images with
resolutions of 722 and 2242. Additionally, the Top-5 accuracy
is significantly higher than the Top-1 accuracy across all
Fig. 7.
---
Fig. 7.
(a) Comparison of the loss for the proposed model, including the
teacher, student, and distillation processes. (b) Training accuracy comparison
of the teacher and student models on both the training and validation sets.
TABLE VII
ABLATION STUDY ON VARIOUS CLASS SUBSETS AND IMAGE
RESOLUTIONS, SHOWING TOP-1 AND TOP-5 ACCURACY USING K-FOLD
CROSS-VALIDATION.
Image
Res.
Classes
2-Fold
3-Fold
4-Fold
Top-1
Top-5
Top-1
Top-5
Top-1
Top-5
722
50
95.9
99.3
96.6
99.6
98.6
99.3
100
94.1
99.0
91.3
98.3
94.2
98.1
---
99.6
98.6
99.3
100
94.1
99.0
91.3
98.3
94.2
98.1
200
92.1
97.9
92.8
98.8
94.4
97.9
1282
50
98.6
99.3
96.6
98.6
96.6
99.3
100
95.0
98.6
93.6
99.6
95.0
99.0
200
93.1
98.2
93.9
98.6
95.8
98.8
2242
50
91.3
98.0
96.2
99.1
94.0
99.3
100
87.0
96.3
89.0
97.5
91.0
98.0
200
85.0
95.0
87.0
96.0
89.0
97.0
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and
---
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
---
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
9
Fig. 8. Testing of the proposed model on random test sample images
Fig. 9. SHAP, Integrated Gradients (IG), and Grad-CAM visualizations of the prediction results. The SHAP interpretation highlights influential ridge patterns
with red pixels. The IG attribution mask indicates the most crucial regions for positive predictions (red circles) and areas with no significant impact (blue
---
circles). Grad-CAM further localizes the most discriminative features used by the model, emphasizing high-attention areas with red-yellow hues. Zoom in for
better visibility.
TABLE VIII
ABLATION STUDY ON STOCHASTIC DEPTH, SHOWING A COMPARISON
BETWEEN STATIC AND ADAPTIVE STOCHASTIC DEPTH
Drop Path Rate
Type
Validation Accuracy (%)
0.0
No Dropout
91.20
0.05
Static
91.80
0.1
Static
92.30
0.2
Static
91.00
0.3
Static
90.50
0.5
Static
89.80
Stochastic Depth
Adaptive
96.67
---
0.5
Static
89.80
Stochastic Depth
Adaptive
96.67
experiments, indicating that the correct class is often within
the top 5 predictions, even when it is not the top prediction.
The ablation study presented in the Table VIII highlights
the impact of both static and adaptive stochastic depth on
validation accuracy. Starting with no dropout (drop path rate
0.0), the validation accuracy is 91.20%. As static stochastic
depth is applied with increasing drop path rates, a marginal
---
improvement is observed at lower rates, with the best perfor-
mance occurring at a drop path rate of 0.1, yielding 92.30%
accuracy. However, as the drop path rate increases beyond 0.1,
the accuracy begins to decline, with a drop path rate of 0.5
resulting in the lowest accuracy of 89.80%. This shows that
static stochastic depth can help improve generalization, but
excessive dropout starts to hinder performance. The adaptive
stochastic depth, however, provides a significant boost in
---
performance, achieving a validation accuracy of 96.67%. This
demonstrates that allowing the model to dynamically adjust
the drop probability during training enables it to better learn
the optimal configuration for each layer, preventing overfitting
and enhancing generalization.
An ablation study analyzing the impact of distillation com-
ponents, focusing on tokens (CLS and DIST) and loss func-
tions (Student Loss, Distillation Loss, and Combined Loss) on
---
validation accuracy is presented in Table IX. The baseline ”No
Distill.” configuration, which uses only the CLS token with
student loss, achieves 91.50% accuracy. Adding the combined
loss (”Single Head”) improves accuracy to 93.10%, as the
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and
---
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
---
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
10
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
TABLE IX
ABLATION STUDY ON DISTILLATION COMPONENTS
Distillation
Tokens
Loss
Val. Acc.(%)
CLS
DIST
No Distill.
✓
×
Stu. Loss
91.50
Single Head
✓
×
Combined Loss
93.10
Distill. Loss
✓
✓
Distill. Loss
94.00
Combined Loss
✓
✓
Combined Loss
96.67
Without Distill.
✓
×
Combined Loss
95.30
TABLE X
IMPACT OF DATA AUGMENTATIONS AND TRANSFORMATIONS ON MODEL
PERFORMANCE
Exp
Aug./Trans.
Accuracy (%)
F1-Score (%)
Lighting
Bright
91.8
90.6
Normal
96.6
---
Lighting
Bright
91.8
90.6
Normal
96.6
96.1
Low
90.4
89.7
Rotations
-30°
92.3
91.4
15°
92.8
92.2
30°
91.9
91.1
Occlusion
Small Patch
89.6
88.3
Large Patch
87.4
86.0
combined loss helps the model leverage richer supervision.
Introducing both CLS and DIST tokens with distillation loss
(”Distill. Loss”) further increases accuracy to 94.00%, as the
DIST token enhances the student model’s ability to mimic
the teacher. The best performance, 96.67%, is achieved using
---
both tokens with the combined loss (”Combined Loss”), as
this setup balances student-specific learning and effective
knowledge transfer. Removing the DIST token while retaining
the combined loss (”Without Distill.”) results in a slight drop
to 95.30%, highlighting that the absence of the DIST token
limits the knowledge transfer from the teacher to the student.
Table X demonstrates the model’s robustness under varying
lighting conditions, rotational variations, and occlusions, sim-
---
ulating real-world challenges. The model achieved the highest
accuracy of 96.6% under normal lighting, with minimal accu-
racy drops under bright (91.8%) and low lighting (90.4%),
indicating resilience to illumination changes. For rotational
variations, the model maintained consistent performance, with
accuracies ranging from 91.9% to 92.8%, highlighting its
ability to handle pose changes. Under occlusions, it achieved
89.6% accuracy for small patches and 87.4% for larger ones,
---
demonstrating robustness even with partial obstructions. Over-
all, the results confirm the model’s reliable performance in
diverse and challenging environments.
The performance of our cattle identification model across
various hardware configurations is presented in Table XI.
The results demonstrate that GPUs significantly outperform
CPUs in terms of latency and throughput, with the RTX
3060 achieving the lowest latency of 1.2 ms and the highest
throughput of 892 images/sec, making it the most efficient
---
configuration for high-speed inference. However, this comes at
the cost of higher power consumption (450W). Comparatively,
CPUs exhibit moderate performance, with the AMD Ryzen
5 5600 achieving a latency of 14.2 ms and throughput of
70 images/sec, making it a viable option for balanced power
efficiency. Edge devices like the Raspberry Pi 4, while power-
efficient (7W), have significantly higher latency of 78.3 ms and
lower throughput of 12 images/sec, indicating their limitations
---
for real-time applications. The overall analysis highlights
the trade-offs between performance, power consumption, and
hardware efficiency, with the RTX 3060 proving to be the
most effective for high-speed cattle verification in practical
deployments.
To evaluate the model’s performance on unseen data, we’ve
developed a pipeline that processes image samples and predicts
their correct labels. Here, in Fig. 8(a-d), represent the true
and predicted labels, respectively. Sample images are provided
---
randomly to the trained model, and as mentioned in Data
acquisition section, the number of images in the testing set
is 900, i.e., each class has 3 testing images, yet for clarity, we
present only the top 10 cases of four different testing instances
here to provide a clearer insight into the results. Although
a few predictions are not accurate and can be clearly seen
in the confusion matrix heatmap, the majority are accurately
identified according to their actual classes.
---
identified according to their actual classes.
The results of the proposed method is compared with
the baseline ViT variants [21], [22], and the parameters
(Params.) are also compared in each case. The number of
heads is basically attention heads in each multi-head self-
attention mechanism. Each attention head captures different
aspects of the input data independently, allowing the model
to attend to multiple parts of the input simultaneously. Depth
denotes the total number of Transformer layers in the ViT
---
architecture. Every layer contains multi-head self-attention
mechanisms along with feed-forward neural networks which
is basically the Transformer blocks and its capacity to capture
complex patterns and relationships within the input data. Width
indicates the dimensionality of the projected embeddings for
tokens (patches) in the input image. The projection dimension
determines the size of the embeddings representing each patch
and influences the model’s ability to capture information from
---
the input image. The results displayed in Table XIII illustrate
better accuracy compared to all other ViT variants. The head
count is kept to a minimum due to the very subtle inter-class
differences in muzzle patterns, while the depth is maximized
to extract the most important muzzle features. Additionally,
the width is minimized to maintain a lower embedding size
that keeps the parameters within limits.
The
Table
XIV
presents
a
comparison
of
the
pro-
---
The
Table
XIV
presents
a
comparison
of
the
pro-
posed method, CattleDiT, with several state-of-the-art face
recognition methods such as ArcFace, CosFace, MagFace,
SphereFace, and ElasticFace. The metrics used for compar-
ison include Precision, Recall, F1-Score, and Accuracy. From
the results, it can be observed that CattleDiT significantly
outperforms all other methods across all metrics, with an
accuracy of 96.67%. This shows that despite the niche task
---
of cattle identification using muzzle images, CattleDiT not
only adapts well but also outperforms techniques that were
originally designed for more general facial recognition tasks
like person-reID or facial identification. These results reinforce
the efficacy of our distillation-based approach with Adaptive
Stochastic Depth in this highly specific application, providing
the community with insights into how these techniques can be
adapted successfully to niche identification tasks.
---
A detailed comparison between the proposed distillation-
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
11
TABLE XI
PERFORMANCE COMPARISON ON DIFFERENT HARDWARE
Hardware
Latency (ms)
Throughput (images/sec)
Power (W)
CPU/GPU Utilization
RAM (MB)
CPU (Intel i5-8600)
19.8
50
35W
90%
620
CPU (Intel i7-9700)
12.3
81
25W
85%
512
GPU (RTX 3050)
2.1
476
220W
40%
1024
GPU (RTX 3060)
1.2
892
450W
35%
2048
Edge (Raspberry Pi 4)
78.3
12
7W
98%
380
CPU (AMD Ryzen 5 5600)
14.2
70
30W
88%
550
TABLE XII
---
14.2
70
30W
88%
550
TABLE XII
COMPARISON OF THE PERFORMANCE OF THE PROPOSED CUSTOM CNN MODEL AS A FEATURE EXTRACTOR WITH DIFFERENT CNN MODELS
Models
Accuracy (%)
Precision (%)
Recall (%)
F1-Score (%)
MCC (%)
Parameters
Inference
ResNet50
85.66
88.23
85.31
86.77
82.76
23.5M
95ms
Inception V3
96.60
94.12
93.34
94.54
95.23
22M
87ms
Xception
98.10
98.23
97.43
97.21
96.65
21M
90ms
MobileNet
71.85
78.33
71.73
72.23
73.04
3.5M
46ms
DenseNet
79.71
79.23
83.32
79.11
78.56
7.9M
53ms
EfficientNet B0
85.19
87.23
84.17
---
78.56
7.9M
53ms
EfficientNet B0
85.19
87.23
84.17
84.29
77.89
3.5M
47ms
EfficientNet B5
92.87
94.45
92.34
93.21
89.56
30M
96ms
AlexNet
73.57
77.37
73.45
72.65
75.51
61M
130ms
VGG
84.09
86.34
84.53
83.67
83.48
138M
220ms
SqueezeNet
85.19
88.34
85.45
84.47
85.39
1.2M
33ms
RegNetX 400MF
78.23
82.63
78.45
78.71
78.37
5.6M
49ms
RegNetY 400MF
89.85
91.56
89.17
89.12
88.01
4.3M
47ms
ConvNeXt-Tiny
92.12
93.89
91.78
92.33
89.01
28M
83ms
ConvNeXt-Small
94.56
96.34
94.21
95.11
92.23
50M
110ms
Proposed (CNN)
96.67
---
94.21
95.11
92.23
50M
110ms
Proposed (CNN)
96.67
98.61
96.59
97.12
95.29
0.13M
30ms
TABLE XIII
PERFORMANCE COMPARISON OF THE PROPOSED METHOD WITH
DIFFERENT VARIANTS OF VISION TRANSFORMERS
Models
Heads
Depth
Width
Acc. (%)
Params.
ViT-Ti [21]
3
8
192
95.11
6.5M
ViT-S [21]
6
8
384
95.37
22M
ViT-B [21]
6
6
768
93.30
86M
DiT [35]
12
12
768
93.50
110M
LeViT [36]
4
12
256
88.77
40M
T2T-ViT [37]
6
14
384
90.53
21.5M
BEiT [38]
12
12
768
91.50
110M
CrossViT [39]
12
12
384
90.80
43.5M
Swin-S [40]
12
12
384
94.87
50M
---
384
90.80
43.5M
Swin-S [40]
12
12
384
94.87
50M
Swin-B [40]
12
12
512
94.70
88M
DeiT-S [22]
6
12
384
93.55
22M
DeiT-B [22]
12
12
768
95.30
86M
CattleDiT
3
12
192
96.67
25M
TABLE XIV
ACCURACY COMPARISON WITH STATE-OF-THE-ART FACE RECOGNITION
METHODS
Methods
Prec.(%)
Rec.(%)
F1-Score(%)
Acc.(%)
Arcface [41]
92.12
91.90
92.01
91.90
Cosface [42]
91.88
91.10
91.49
91.10
MagFace [43]
92.75
91.65
92.20
91.65
SphereFace [44]
93.25
92.55
92.89
92.55
ElasticFace [45]
94.30
93.75
94.02
93.75
CattleDiT
98.61
96.59
---
94.30
93.75
94.02
93.75
CattleDiT
98.61
96.59
97.12
96.67
based transformer and state-of-the-art CNN models trained
as teacher models is shown in Table XII. It can be inferred
from the results that accuracy is higher in the case of teachers
trained with a custom lightweight CNN model. Although some
models, such as ConvNeXt-Small and EfficientNet B5, demon-
strate higher accuracy levels, they also require significantly
higher parameters and more inference time (in milliseconds)
TABLE XV
---
TABLE XV
COMPARISON OF THE ACCURACY WITH DIFFERENT DATASETS PUBLICLY
AVAILABLE
Datasets
#Classes
#images
Resolution
Accuracy (%)
Li et al. [46]
230
2480
1282
91.33
2242
92.88
Ahmed et al. [47]
449
4456
1282
71.40
2242
77.19
This study [48]
300
3892
1282
96.67
2242
95.40
Ours
50
1248
1282
94.89
2242
95.12
when testing individual samples. Conversely, our distillation-
based method, accompanied by the proposed custom CNN
encoder, achieves comparable or superior accuracy with signif-
---
icantly fewer parameters and less inference time on the same
training data. We have also calculated the Matthews Correla-
tion Coefficient (MCC), which offers a balanced evaluation of
the classifier’s performance by considering all four confusion
matrix terms (accuracy, precision, recall, and F1-score). It can
be observed here that the proposed student model performs
comparatively better when using a lightweight CNN as a
backbone, even outperforming several state-of-the-art models
in terms of efficiency.
---
in terms of efficiency.
The results shown in Table XV represent the accuracy
obtained on various publicly available datasets compared to
the datasets used in our study. All datasets were processed
using the same preprocessing techniques applied in our work.
The accuracy obtained on other datasets is comparatively lower
due to the image’s poor quality of the muzzle area, which often
includes much larger occlusion, poor lighting conditions, and
muzzles covered with grass. Additionally, we have included
---
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
12
IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE
a new dataset consisting of cattle front-facing videos with a
focus on the muzzle area, containing 50 classes. The accuracy
achieved on this dataset was 94.89%, highlighting the robust-
ness of our model for real-time monitoring and recognition
applications. This dataset adds a new dimension to our study,
emphasizing the capability of the model to handle moving
targets in real-world scenarios.
TABLE XVI
---
targets in real-world scenarios.
TABLE XVI
COMPARISON OF ACCURACY WITH OTHER STATE-OF-THE-ART METHODS
APPLIED ON MUZZLE IMAGES
Ref.
Accuracy (%)
Feature
Methods
Lee et al. [15]
85.78
Muzzle
EfficientNetV2
Shojaeipour et al. [4]
87.10
Muzzle
ResNet50(Tuned)
Awad et al. [2]
85.50
Muzzle
SIFT, SURF
Kumar et al. [3]
76.40
Muzzle
SURF, IDT
Noviyanto et al. [7]
73.71
Muzzle
SIFT
Gaber et al. [9]
81.59
Face
WLD, AdaBoost
Ruchay et al. [49]
93.72
Face
VGGFace2
Sanjel et al. [50]
94.17
Muzzle
VGGFace
---
VGGFace2
Sanjel et al. [50]
94.17
Muzzle
VGGFace
Meng et al. [51]
93.78
Face
ViT-L, AM-Softmax
Zhang et al. [52]
90.84
Face
Eff. Net, D. Otsu
Proposed
96.67
Muzzle
CattleDiT
Finally, cattle identification accuracy has been compared
with other methods applied to muzzle and face images and
shown in Table XVI. We have kept the preprocessed data
consistent across all the cases, but we utilized the model
proposed by the mentioned authors to evaluate the results,
---
and then finally compared it with our proposed method. The
results show that CattleDiT achieves the highest accuracy
of 96.67%, outperforming all other methods. This demon-
strates the effectiveness of CattleDiT, specifically optimized
for muzzle-based identification, surpassing even face-based
models like VGGFace and ViT-L, which achieve high accuracy
but fall short in comparison. The table also highlights the
evolution from traditional feature extraction methods such as
---
SIFT and SURF to advanced deep learning techniques like
EfficientNetV2, VGGFace, and ResNet50.
V. CONCLUSION AND FUTURE WORK
This research illustrates the non-invasive identification of
individual cattle using a novel deep-learning approach. This
proposed approach consists of detection and cropping of the
muzzle, a lightweight CNN model, an Vision Transformer
model, and a knowledge distillation process where a teacher
model is trained on a lightweight CNN model and then
---
merged with a student transformer model having an Adaptive-
stochastic depth mechanism and token mix process. To validate
the proposed model experimentally, a dataset of cattle faces
is collected from a public source containing front-facing
images of 300 cattle subjects. Initially, the muzzle part was
extracted from the front face using the state-of-the-art Yolo
v10 algorithm, followed by augmentation and transformation
to increase the image diversity and introduce variations. This
---
CattleDiT model exhibits strong performance on our test
images, showing higher accuracy compared to conventional
Vision Transformer models and also hardware efficiency. We
conducted a comparative analysis with various variants of ViT
models and state-of-the-art CNN models.
While our novel approach shows promising results on a
public dataset, it limits the generalizability of the model.
Hence, there is a potential need for training on diverse datasets
---
encompassing different cattle breeds, and it may face chal-
lenges in real-world scenarios with varying lighting conditions,
occlusions, and environmental factors, which could affect the
performance of the muzzle detection and cropping stage. To
overcome these limitations, it is imperative to explore the
adaptation of the model to different environments and cattle
populations, thereby enhancing its robustness and applicability.
Investigating techniques to mitigate the impact of environmen-
---
tal factors on image quality and implementing real-time mon-
itoring capabilities could further advance the practical utility
of the proposed approach. Integrating additional features such
as behavioral characteristics or body markings could enrich
the identification process, leading to more comprehensive and
accurate cattle recognition systems.
ACKNOWLEDGEMENT
We gratefully acknowledge the support from the PARAM
Shivay Facility under the National Supercomputing Mission,
---
Government of India, at IIT (BHU), Varanasi, and the fund-
ing from the NASF project ‘Artificial Intelligence & IoT-
based Smart Vet Ecosystem for Animal Health, Patient Care,
and Precision Livestock Farming’ (Grant No. NASF/PA-
9028/2022-23).
REFERENCES
[1] B. Xu, W. Wang, L. Guo, G. Chen, Y. Li, Z. Cao, and S. Wu.
Cattlefacenet: A cattle face identification approach based on retinaface
and arcface loss. Computers and Electronics in Agriculture, 193:106675,
2022.
---
2022.
[2] A. I. Awad and M. Hassaballah. Bag-of-visual-words for cattle identi-
fication from muzzle print images. Applied Sciences, 9(22):4914, 2019.
[3] S. Kumar, M. K. Chaube, and S. Kumar.
Secure and sustainable
framework for cattle recognition using wireless multimedia networks
and machine learning techniques.
IEEE Transactions on Sustainable
Computing, 7(3):696–708, 2022.
[4] A. Shojaeipour, G. Falzon, P. Kwan, N. Hadavi, F. C. Cowley, and
---
D. Paul. Automated muzzle detection and biometric identification via
few-shot deep transfer learning of mixed breed cattle. Agronomy, 11(11),
2021.
[5] W. Hao, K. Zhang, M. Han, W. Hao, J. Wang, F. Li, and Z. Liu. A novel
jinnan individual cattle recognition approach based on mutual attention
learning scheme. Expert Systems with Applications, 230:120551, 2023.
[6] R. Li, Y. Wen, S. Zhang, X. Xu, B. Ma, and H. Song. Automated mea-
surement of beef cattle body size via key point detection and monocular
---
depth estimation. Expert Systems with Applications, 244:123042, 2024.
[7] A. Noviyanto and A. M. Arymurthy. Beef cattle identification based on
muzzle pattern using a matching refinement technique in the sift method.
Computers and Electronics in Agriculture, 99:77–84, 2013.
[8] W. Andrew, S. Hannuna, N. Campbell, and T. Burghardt. Automatic
individual holstein friesian cattle identification via selective local coat
pattern matching in rgb-d imagery. In 2016 IEEE International Confer-
---
ence on Image Processing (ICIP), pages 484–488, 2016.
[9] T. Gaber, A. Tharwat, A. E. Hassanien, and V. Snasel. Biometric cattle
identification approach based on weber’s local descriptor and adaboost
classifier. Computers and Electronics in Agriculture, 122:55–66, 2016.
[10] S. Kumar, A. Pandey, K. S. R. Satwik, S. Kumar, S. K. Singh, A. K.
Singh, and A. Mohan. Deep learning framework for recognition of cattle
using muzzle point image pattern. Measurement, 116:1–17, 2018.
---
[11] A. Bhole, S. S. Udmale, O. Falzon, and G. Azzopardi. Corf3d contour
maps with application to holstein cattle recognition from rgb and thermal
images. Expert Systems with Applications, 192:116354, 2022.
[12] R. Dulal, L. Zheng, M. A. Kabir, S. McGrath, J. Medway, D. Swain,
and W. Swain. Automatic cattle identification using yolov5 and mosaic
augmentation: A comparative analysis. 2022.
---
augmentation: A comparative analysis. 2022.
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
---
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
KUMAR et al.: CATTLEDIT: A DISTILLATION-DRIVEN TRANSFORMER FOR CATTLE IDENTIFICATION
13
[13] H. Gong, H. Pan, L. Chen, T. Hu, S. Li, Y. Sun, Y. Mu, Y. Guo,
and J. Gou. Facial recognition of cattle based on sk-resnet. Scientific
Programming, 2022:5773721, 2022.
[14] Z. Weng, F. Meng, S. Liu, Y. Zhang, Z. Zheng, and C. Gong. Cattle
face recognition based on a two-branch convolutional neural network.
Computers and Electronics in Agriculture, 196:106871, 2022.
---
[15] Taejun Lee, Youngjun Na, Beob Gyun Kim, Sangrak Lee, and Yongjun
Choi.
Identification of individual hanwoo cattle by muzzle pattern
images through deep learning. Animals, 13(18), 2023.
[16] Xingshi Xu, Hongxing Deng, Yunfei Wang, Shujin Zhang, and Huaibo
Song. Boosting cattle face recognition under uncontrolled scenes by
embedding enhancement and optimization.
Applied Soft Computing,
164:111951, 2024.
[17] Jianxing Xiao, Yongsheng Si, Meiling Xie, Gang Liu, Zhang Yan, and
---
Kejian Wang. A novel and convenient lying cow identification method
based on yolox and cowbodynet: A study with applications in a barn.
Computers and Electronics in Agriculture, 225:109287, 2024.
[18] Z. Li, X. Lei, and S. Liu. A lightweight deep learning model for cattle
face recognition. Computers and Electronics in Agriculture, 195:106848,
2022.
[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. arXiv preprint, 2015.
---
recognition. arXiv preprint, 2015.
[20] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking
the inception architecture for computer vision. arXiv preprint, 2015.
[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby.
An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint, 2021.
[22] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
---
H. J´egou. Training data-efficient image transformers distillation through
attention. arXiv preprint, 2021.
[23] M. Shahchera and H. Ebrahimpour-Komleh.
Deit model for iranian
traffic sign recognition in advanced driver assistance systems. In 2023
6th International Conference on Pattern Recognition and Image Analysis
(IPRIA), pages 1–6, 2023.
[24] Y. Yin, W. Jin, J. Bai, R. Liu, and H. Zhen.
Smil-deit: Multiple
instance learning and self-supervised vision transformer network for
---
early alzheimer’s disease classification.
In 2022 International Joint
Conference on Neural Networks (IJCNN), pages 1–6, 2022.
[25] A. Alotaibi, T. Alafif, F. Alkhilaiwi, Y. Alatawi, H. Althobaiti, A. Alre-
faei, Y. Hawsawi, and T. Nguyen. Vit-deit: An ensemble model for breast
cancer histopathological images classification. In 2023 1st International
Conference on Advanced Innovations in Smart Cities (ICAISC), pages
1–6, 2023.
[26] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural
---
network. arXiv preprint, 2015.
[27] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,
Alexandre Sablayrolles, and Herv´e J´egou. Training data-efficient image
transformers & distillation through attention. In International Confer-
ence on Machine Learning, pages 10347–10357. PMLR, 2021.
[28] Byeongho Heo, Minsik Lee, Seunghyun Yun, Jin Young Choi, and
Youngjung Choi. A comprehensive overhaul of feature distillation. arXiv
preprint arXiv:1904.01866, 2019.
---
preprint arXiv:1904.01866, 2019.
[29] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look
once: Unified, real-time object detection. arXiv preprint, 2016.
[30] D. Reis, J. Kupec, J. Hong, and A. Daoudi. Real-time flying object
detection with yolov8. 2024.
[31] C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao. Yolov7: Trainable
bag-of-freebies sets new state-of-the-art for real-time object detectors.
2022.
[32] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and
---
Guiguang Ding. Yolov10: Real-time end-to-end object detection, 2024.
[33] S. Lundberg and S.-I. Lee. A unified approach to interpreting model
predictions. arXiv preprint, 2017.
[34] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep
networks.
In Proceedings of the 34th International Conference on
Machine Learning - Volume 70 (ICML’17), pages 3319–3328, 2017.
[35] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu
---
Wei. Dit: Self-supervised pre-training for document image transformer.
arXiv preprint, 2022.
[36] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand
Joulin, Herv´e J´egou, and Matthijs Douze. Levit: A vision transformer
in convnet’s clothing for faster inference. arXiv preprint, 2021.
[37] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang
Jiang, Francis E. H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-
token vit: Training vision transformers from scratch on imagenet. arXiv
---
preprint, 2021.
[38] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-
training of image transformers. arXiv preprint, 2022.
[39] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda.
Crossvit: Cross-
attention multi-scale vision transformer for image classification. arXiv
preprint, 2021.
[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision
transformer using shifted windows. arXiv preprint, 2021.
---
[41] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and
Stefanos Zafeiriou.
Arcface: Additive angular margin loss for deep
face recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(10):5962–5979, October 2022.
[42] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao
Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for
deep face recognition, 2018.
[43] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface:
---
A universal representation for face recognition and quality assessment,
2021.
[44] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and
Le Song. Sphereface: Deep hypersphere embedding for face recognition,
2018.
[45] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper.
Elasticface: Elastic margin loss for deep face recognition, 2022.
[46] G. Li, G. E. Erickson, and Y. Xiong. Individual beef cattle identification
using muzzle images and deep learning techniques. Animals (Basel),
---
12(11):1453, Jun 2022.
[47] S. U. Ahmed, J. Frnda, M. Waqas, and M. H. Khan. Dataset of cattle
biometrics through muzzle images. Data in Brief, 53:110125, 2024.
[48] Cattle face image dataset for biometric identification. https://cloud.une.
edu.au/index.php/s/eMwaHAPK08dCDru, 2021.
[49] Alexey Ruchay, Vladimir Kolpakov, Guo Hao, and Andrea Pezzuolo.
On-barn cattle facial recognition using deep transfer learning and data
augmentation. Computers and Electronics in Agriculture, 225:109306,
07 2024.
---
07 2024.
[50] Arun Sanjel, Bikram Khanal, Pablo Rivas, and Gregory Speegle. Non-
invasive muzzle matching for cattle identification using deep learning.
07 2023.
[51] Yao Meng, Sook Yoon, Shujie Han, Alvaro Fuentes, Jongbin Park,
Yongchae Jeong, and Dong Park. Improving known–unknown cattle’s
face recognition for smart livestock farm management.
Animals,
13:3588, 11 2023.
[52] Ruihong Zhang, Jiangtao Ji, Kaixuan Zhao, Jinjin Wang, Meng Zhang,
and Meijia Wang.
A cascaded individual cow identification method
---
A cascaded individual cow identification method
based on deepotsu and efficientnet. Agriculture, 13:279, 01 2023.
Niraj Kumar (Student Member, IEEE) completed
his M.Tech degree in computer science at the Na-
tional Institute of Technology (NIT) in Jamshedpur,
Jharkhand, India, in 2021. Presently, he is actively
pursuing a Ph.D. in the Department of Computer
Science and Engineering at the Indian Institute of
Technology (BHU) in Varanasi, India. His areas of
research interest encompass machine learning, deep
---
learning, computer vision, and data mining.
Sanjay Kumar Singh (Senior Member, IEEE),
holds a B.Tech degree in computer engineering,
an M.Tech. degree in computer applications, and
a Ph.D. in computer science and engineering. He
currently serves as a Professor in the Department
of Computer Science and Engineering at the Indian
Institute of Technology (BHU) in Varanasi, India.
His scholarly contributions encompass over 150 pub-
lications, including national and international journal
---
articles, book chapters, and conference papers. He
plays an active role as a Guest Editorial Board
member and reviewer for reputable international journals.
This article has been accepted for publication in IEEE Transactions on Biometrics, Behavior, and Identity Science. This is the author's version which has not been fully edited and 
content may change prior to final publication. Citation information: DOI 10.1109/TBIOM.2025.3565516
---
© 2025 IEEE. All rights reserved, including rights for text and data mining and training of artificial intelligence and similar technologies. Personal use is permitted,
but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: International Institute of Information Technology-Raipur. Downloaded on May 15,2025 at 08:16:18 UTC from IEEE Xplore.  Restrictions apply.
---
